from fuzzywuzzy import fuzz, process
import os, re, json, glob, io
from datetime import datetime
from typing import List, Dict, Any

import pdfplumber
import pytesseract
from PIL import Image
import fitz  # PyMuPDF

from langchain.schema import Document
from sentence_transformers import SentenceTransformer
from transformers import pipeline
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders.pdf import PyPDFLoader
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import DeepLake

class PDFChunkProcessor:
    KNOWN_COMPANIES = [
        "Dangote Cement", "Maaden", "Chemplast Sanmar", "Dangote Group", "Saudi Aramco", "Maaden", "Chemplast Sanmar", "Dangote", 
        "Nestle", "Unilever", "Tata Steel", "Siemens", "Shell", "Chevron", 
        "ExxonMobil", "TotalEnergies", "Reliance", "BASF", "Volkswagen", "ABB", "Aramco", "Petronas", "Lucky Cement",
        "National Cement Company", "Sharjah Cement & Industrial Development", "Net Carbon Vision"
    ]
    
    def __init__(self, output_dir="output", save_structured=True):
        os.makedirs(output_dir, exist_ok=True)
        self.output_dir = output_dir
        self.save_structured = save_structured

        self.splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        self.embedder = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        self.qa_pipeline = pipeline("question-answering", model="distilbert-base-uncased-distilled-squad")
        self.vectorstore = None

        self.chunk_data = []
        self.chunk_texts = []
        self.keywords = {
            "scope_1": ["scope 1", "direct emissions", "stationary combustion", "fuel use", "company-owned vehicles"],
            "scope_2": ["scope 2", "indirect emissions", "purchased electricity", "grid emissions"],
            "scope_3": ["scope 3", "value chain emissions", "upstream", "downstream", "supply chain", "business travel"],
            "environment": ["environmental", "sustainable", "climate", "eco", "carbon", "greenhouse", "emissions"],
            "social": ["social", "employee", "diversity", "inclusion", "human rights", "safety"],
            "governance": ["governance", "board", "ethics", "policy", "regulation"],
            "financial": ["revenue", "profit", "investment", "dividend"],
            "energy_sector": ["coal", "cement", "oil", "gas", "steel", "manufacturing", "mining", "refinery"]
        }

    def clean_filename_name(self, filename: str) -> str:
        name = os.path.splitext(os.path.basename(filename))[0]
        name = re.sub(r'[_\-]', ' ', name)
        name = re.sub(r'(sustainability|report|ESG|AR|CR|CSR|[0-9]{2,4})', '', name, flags=re.IGNORECASE)
        return name.strip()

    def fuzzy_extract_company_name(self, name_from_filename: str) -> str:
        name_from_filename = name_from_filename.lower()
        best_match = process.extractOne(name_from_filename, self.KNOWN_COMPANIES, scorer=fuzz.partial_ratio)
        if best_match and best_match[1] > 80:
            return best_match[0]
        return None

    def extract_company_from_pdf_content(self, text: str) -> str:
        text = text.lower()
        best_match = process.extractOne(text, self.KNOWN_COMPANIES, scorer=fuzz.partial_ratio)
        if best_match and best_match[1] > 80:
            return best_match[0]
        return None

    def extract_metadata(self, text: str, filename: str = "") -> Dict[str, str]:
        name_from_filename = self.clean_filename_name(filename)
        company_from_filename = self.fuzzy_extract_company_name(name_from_filename)
        company_from_content = self.extract_company_from_pdf_content(text)
        company_name = company_from_filename or company_from_content or "Unknown Company"
    
        year_match = re.search(r'\b(19|20)\d{2}\b', text)
        if not year_match:
            year_match = re.search(r'\b(19|20)\d{2}\b', filename)
        year = year_match.group(0) if year_match else "Unknown Year"
    
        return {
            "company_name": company_name,
            "report_year": year,
            "company": self.extract_organizations(text)  
        }

    def extract_organizations(self, text: str) -> List[str]:
        pattern = r'\b(?:[A-Z][a-zA-Z&,.\-]+(?: [A-Z][a-zA-Z&,.\-]+)*)(?: (Inc\.?|Ltd\.?|LLC|Group|Company|Corp\.?|PLC|AG|SA|GmbH))\b'
        matches = re.findall(pattern, text)
        return list(set([m.strip() for m in matches if "\n" not in m and len(m.split()) > 1]))[:5]

    def process_pdf(self, pdf_dir: str, pdf_name: str = None) -> List[Dict[str, Any]]:
        files = [os.path.join(pdf_dir, pdf_name)] if pdf_name else glob.glob(os.path.join(pdf_dir, "**/*.pdf"), recursive=True)
        raw_docs = []
        extras = []

        for path in files:
            try:
                raw_docs += PyPDFLoader(path).load()
                table_ocr_data = self.extract_tables_images(path)
                extras = [Document(page_content=d["content"], metadata={"source": path}) for d in table_ocr_data]
                raw_docs += extras

                structured_output_path = os.path.join(self.output_dir, f"{os.path.splitext(os.path.basename(path))[0]}_tables_images.json")
                with open(structured_output_path, "w", encoding="utf-8") as f:
                    json.dump(table_ocr_data, f, indent=2)
                print(f"📊 Tables/Images saved to: {structured_output_path}")

            except Exception as e:
                print(f"❌ Failed to load {path}: {e}")

        if not raw_docs:
            return []

        meta = self.extract_metadata(" ".join(d.page_content for d in raw_docs[:5]), filename=os.path.basename(files[0]))
        chunks = self.splitter.split_documents(raw_docs)
        self.chunk_texts = [c.page_content for c in chunks]
        self.chunk_data = [{
            "chunk_id": i,
            "content": c.page_content,
            "metadata": {
                "source": os.path.basename(c.metadata.get("source", "unknown")),
                "page": c.metadata.get("page"),
                "chunk_index": i,
                "word_count": len(c.page_content.split()),
                "extracted_dates": self.extract_dates(c.page_content),
                "company": self.extract_organizations(c.page_content),  
                "tags": self.extract_tags(c.page_content),
                "processing_timestamp": datetime.now().isoformat(),
                **meta
            }
            } for i, c in enumerate(chunks)]

        self.vectorstore = DeepLake.from_texts(
            texts=self.chunk_texts,
            embedding=self.embedder,
            dataset_path=os.path.join(self.output_dir, "deeplake_vectorstore"),
            metadatas=[d["metadata"] for d in self.chunk_data]
        )

        print(f"✅ {len(self.chunk_data)} chunks processed.")
        pdf_basename = os.path.splitext(os.path.basename(files[0]))[0]
        per_file_path = os.path.join(self.output_dir, f"{pdf_basename}_chunks.json")
        with open(per_file_path, "w", encoding="utf-8") as f:
            json.dump(self.chunk_data, f, indent=2)
        print(f"📁 Individual file chunks saved to: {per_file_path}")
        return self.chunk_data

    def extract_dates(self, text: str) -> List[str]:
        patterns = [
            r'\b\d{4}-\d{2}-\d{2}\b', r'\b\d{2}/\d{2}/\d{4}\b',
            r'\b\d{4}/\d{2}/\d{2}\b', r'\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* \d{1,2}, \d{4}\b',
            r'\b\d{1,2} (?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* \d{4}\b',
            r'\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* \d{4}\b',
            r'\b(19|20)\d{2}\b'
        ]
        return list(set([match for p in patterns for match in re.findall(p, text)]))[:10]

    def extract_tags(self, text: str) -> List[str]:
        keywords = {
            "scope_1": ["scope 1", "direct emissions", "stationary combustion", "fuel use", "company-owned vehicles"],
            "scope_2": ["scope 2", "indirect emissions", "purchased electricity", "grid emissions"],
            "scope_3": ["scope 3", "value chain emissions", "upstream", "downstream", "supply chain", "business travel"],
            "environment": ["environmental", "sustainable", "climate", "eco", "carbon", "greenhouse", "emissions"],
            "social": ["social", "employee", "diversity", "inclusion", "human rights", "safety"],
            "governance": ["governance", "board", "ethics", "policy", "regulation"],
            "financial": ["revenue", "profit", "investment", "dividend"],
            "energy_sector": ["coal", "cement", "oil", "gas", "steel", "manufacturing", "mining", "refinery"]
        }
        text = text.lower()
        return [cat for cat, keys in keywords.items() if any(k in text for k in keys)]

    def extract_tables_images(self, path: str) -> List[Dict[str, Any]]:
        structured_results = []

        with pdfplumber.open(path) as pdf:
            for page_num, page in enumerate(pdf.pages):
                tables = page.extract_tables()
                for table in tables:
                    table_clean = [list(filter(None, row)) for row in table if any(row)]
                    table_text = "\n".join(" | ".join(row) for row in table_clean)
                    structured_results.append({
                        "type": "table",
                        "page": page_num + 1,
                        "content": table_text,
                        "table_data": table_clean
                    })

        doc = fitz.open(path)
        for page_index in range(len(doc)):
            page = doc[page_index]
            for img in page.get_images(full=True):
                try:
                    xref = img[0]
                    base_image = doc.extract_image(xref)
                    image = Image.open(io.BytesIO(base_image["image"])).convert("RGB")
                    ocr_text = pytesseract.image_to_string(image)
                    if ocr_text.strip():
                        structured_results.append({
                            "type": "image_ocr",
                            "page": page_index + 1,
                            "content": ocr_text.strip(),
                            "ocr_text": ocr_text.strip()
                        })
                except Exception as e:
                    print(f"⚠️ Failed to OCR image on page {page_index + 1}: {e}")

        return structured_results

    def save_chunks(self, prefix="pdf_chunks") -> str:
        path = os.path.join(self.output_dir, f"{prefix}_all.json")
        with open(path, "w", encoding="utf-8") as f:
            json.dump(self.chunk_data, f, indent=2)
        print(f"📄 Chunks saved to {path}")
        return path

    def is_comparison_question(self, question: str) -> bool:
        question = question.lower()
        comparison_patterns = [
            r'\bcompare\b', 
            r'\bversus\b', 
            r'\bvs\b', 
            r'\bdifference between\b', 
            r'\bcompare.*between\b',
            r'\bhow.*compare\b',
            r'\bwhich.*higher\b',
            r'\bwhich.*lower\b',
            r'\bdifferences\b'
        ]
        return any(re.search(pattern, question) for pattern in comparison_patterns)

    def answer(self, question: str, top_k: int = 5) -> Dict[str, Any]:
        if not self.vectorstore:
            raise ValueError("Run process_pdf first.")
        
        if self.is_comparison_question(question):
            print("\n🔎 Detected comparison question. Running comparison...")
            return self.compare_information(question)
        
        results = self.vectorstore.similarity_search(question, k=top_k)
        answers = []
        for r in results:
            try:
                res = self.qa_pipeline(question=question, context=r.page_content)
                answer_text = res.get("answer", "No answer")
                score = res.get("score", 0)
                context = r.page_content[:500]  
                metadata = r.metadata
                
                start = context.lower().find(answer_text.lower())
                end = start + len(answer_text)
                highlighted_context = (
                    context[:start] + 
                    f"[{context[start:end]}]" + 
                    context[end:]
                )
                
                answers.append({
                    "answer": answer_text,
                    "score": score,
                    "context": highlighted_context,
                    "source": metadata.get("source", "Unknown"),
                    "page": metadata.get("page", "Unknown"),
                    "chunk_index": metadata.get("chunk_index", "Unknown"),
                    "word_count": metadata.get("word_count", "Unknown"),
                    "extracted_dates": metadata.get("extracted_dates", []),
                    "company": self.extract_companies_from_question(question),  # Extract company name from the question
                    "tags": metadata.get("tags", []),
                    "processing_timestamp": metadata.get("processing_timestamp", "Unknown")
                })
            except Exception as e:
                answers.append({
                    "answer": "QA Error",
                    "score": 0,
                    "context": f"Error processing answer: {str(e)}",
                    "source": "Unknown",
                    "page": "Unknown",
                    "chunk_index": "Unknown",
                    "word_count": "Unknown",
                    "extracted_dates": [],
                    "company": self.extract_companies_from_question(question),  # Extract company name from the question
                    "tags": [],
                    "processing_timestamp": "Unknown"
                })
        
        return {
            "question": question,
            "answers": answers
        }

    def extract_companies_from_question(self, question: str) -> List[str]:
        companies = []
        for company in self.KNOWN_COMPANIES:
            if company.lower() in question.lower():
                companies.append(company)
        return companies

    def extract_years_from_question(self, question: str) -> List[str]:
        year_pattern = r'\b(19|20)\d{2}\b'
        years = re.findall(year_pattern, question)
        return years

    def extract_emission_scope_from_question(self, question: str) -> str:
        scope_keywords = {
            "scope_1": ["scope 1", "direct emissions"],
            "scope_2": ["scope 2", "indirect emissions"],
            "scope_3": ["scope 3", "value chain emissions"]
        }
        for scope, keywords in scope_keywords.items():
            for keyword in keywords:
                if keyword.lower() in question.lower():
                    return scope
        return "unknown"

    def compare_information(self, question: str) -> Dict[str, Any]:
        comparison_results = []
        
        companies = self.extract_companies_from_question(question)
        years = self.extract_years_from_question(question)
        emission_scope = self.extract_emission_scope_from_question(question)
        
        relevant_terms = []
        for key_category, keywords in self.keywords.items():
            for keyword in keywords:
                if keyword.lower() in question.lower():
                    relevant_terms.append(keyword)
        
        for company in companies:
            for year in years:
                query = f"{company} {year} {emission_scope.replace('_', ' ')} emissions {' '.join(relevant_terms[:5])}"
                results = self.vectorstore.similarity_search(
                    query, 
                    k=5,
                    filter={
                        "metadata": {
                            "company_name": {"$eq": company},
                            "report_year": {"$eq": year},
                            "tags": {"$in": [emission_scope] if emission_scope != "unknown" else self.keywords.keys()}
                        }
                    }
                )
                
                if results:
                    most_relevant = results[0]
                    metadata = most_relevant.metadata
                    
                    emission_pattern = r'(\d+(?:,\d+)*(?:\.\d+)?)(?:\s*(?:tons?|t|kt|mt|kg)?(?:\s*co2e?)?)'
                    matches = re.findall(emission_pattern, most_relevant.page_content)
                    
                    relevant_tags = set(metadata.get("tags", []))
                    context_snippet = most_relevant.page_content[:300]
                    
                    comparison_results.append({
                        "company": company,
                        "year": year,
                        "emission_scope": emission_scope,
                        "value": matches[0] if matches else "Not found",
                        "context": context_snippet,
                        "page": metadata.get("page", "Unknown"),
                        "source": metadata.get("source", "Unknown"),
                        "extracted_dates": metadata.get("extracted_dates", []),
                        "relevant_tags": list(relevant_tags),
                        "organizations_mentioned": metadata.get("potential_organizations", [])
                    })
                else:
                    fallback_query = f"{company} {emission_scope.replace('_', ' ')} emissions"
                    fallback_results = self.vectorstore.similarity_search(fallback_query, k=3)
                    
                    if fallback_results:
                        context = fallback_results[0].page_content
                        comparison_results.append({
                            "company": company,
                            "year": year,
                            "emission_scope": emission_scope,
                            "value": "Year-specific data not found",
                            "context": context[:300],
                            "page": fallback_results[0].metadata.get("page", "Unknown"),
                            "source": fallback_results[0].metadata.get("source", "Unknown")
                        })
                    else:
                        comparison_results.append({
                            "company": company,
                            "year": year,
                            "emission_scope": emission_scope,
                            "value": "No data found",
                            "context": "No relevant information available"
                        })
        
        comparison_metrics = {}
        if comparison_results and all(r["value"] != "No data found" and r["value"] != "Year-specific data not found" for r in comparison_results):
            try:
                values = [float(r["value"].replace(",", "")) for r in comparison_results]
                comparison_metrics = {
                    "highest": comparison_results[values.index(max(values))]["company"],
                    "lowest": comparison_results[values.index(min(values))]["company"],
                    "difference_percentage": round((max(values) - min(values)) / min(values) * 100, 2) if min(values) > 0 else None
                }
            except (ValueError, IndexError):
                pass
        
        return {
            "question": question,
            "comparison_results": comparison_results,
            "comparison_metrics": comparison_metrics,
            "identified_terms": {
                "companies": companies,
                "years": years,
                "emission_scope": emission_scope,
                "relevant_keywords": relevant_terms
            }
        }

def main():
    current_dir = os.getcwd()
    processor = PDFChunkProcessor(os.path.join(current_dir, "output"))
    
    default_dir = current_dir
    dir_path = input(f"📂 Enter PDF directory (default: {default_dir}): ").strip()
    if not dir_path:
        dir_path = default_dir
        print(f"Using default directory: {dir_path}")
        
    process_all = input("🔄 Process all PDFs? (y/n): ").lower() == "y"
    selected_files = []
    all_chunks = []

    if process_all:
        pdf_file = None
    else:
        all_pdfs = sorted(glob.glob(os.path.join(dir_path, "*.pdf")))
        if not all_pdfs:
            print("❌ No PDFs found in directory.")
            return

        print("\n🔘 Choose an option:")
        print("  1. Process first N PDFs")
        print("  2. Manually select PDF files")
        print("  3. Cancel")

        choice = input("Enter choice (1/2/3): ").strip()
        if choice == "1":
            try:
                num_files = int(input(f"📑 {len(all_pdfs)} PDFs found. How many would you like to process? ").strip())
                selected_files = all_pdfs[:num_files]
            except ValueError:
                print("❌ Invalid number entered.")
                return
        elif choice == "2":
            print("\n📋 Available PDFs:")
            for idx, file in enumerate(all_pdfs):
                print(f"  [{idx+1}] {os.path.basename(file)}")
            file_indices = input("📥 Enter comma-separated numbers of PDFs to process (e.g., 1,3,5): ").strip()
            try:
                indices = [int(i) - 1 for i in file_indices.split(",")]
                selected_files = [all_pdfs[i] for i in indices if 0 <= i < len(all_pdfs)]
            except Exception as e:
                print(f"❌ Error parsing input: {e}")
                return
        elif choice == "3":
            print("🚫 Cancelled.")
            return
        else:
            print("❌ Invalid choice.")
            return

    if process_all:
        chunks = processor.process_pdf(dir_path, None)
        all_chunks.extend(chunks)
    else:
        for path in selected_files:
            print(f"\n📄 Processing: {os.path.basename(path)}")
            chunks = processor.process_pdf(os.path.dirname(path), os.path.basename(path))
            all_chunks.extend(chunks)

    combined_path = os.path.join(processor.output_dir, "all_pdf_chunks.json")
    with open(combined_path, "w", encoding="utf-8") as f:
        json.dump(all_chunks, f, indent=2)
    print(f"\n📦 All chunks saved to {combined_path}")

    while True:
        q = input("\n❓ Ask a question (type 'exit' to quit): ").strip()
        if q.lower() == "exit":
            break
        print(json.dumps(processor.answer(q), indent=2))

if __name__ == "__main__":
    main()
